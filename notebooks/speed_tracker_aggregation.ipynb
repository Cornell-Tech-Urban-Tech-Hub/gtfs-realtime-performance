{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "# changes the current working directory of your Jupyter Notebook (or Python script) to root directory\n",
    "os.chdir(\"/Users/luohy/Documents/Projects/bus-observatory/gtfs-realtime-performance\")\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # Used in load_speed_data function\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speed_parquets(directory: str, start_date: str, end_date: str) -> list[str]:\n",
    "    \"\"\"Find all bus speed parquet files in the directory within date range\"\"\"\n",
    "    speed_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.parquet') and filename != 'stops.parquet':\n",
    "            try:\n",
    "                # Extract date from bus_speeds_20241204.parquet\n",
    "                file_date = filename.split('_')[-1].split('.')[0]\n",
    "                if start_date <= file_date <= end_date:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    speed_files.append(file_path)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    print(f\"Found {len(speed_files)} speed parquets between {start_date} and {end_date} in {directory}\")\n",
    "    return speed_files\n",
    "    \n",
    "\n",
    "def load_speed_parquet_data(file_paths: List[str], max_workers: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"Load and combine multiple speed parquet files in parallel\"\"\"\n",
    "    speed_data_frames = []\n",
    "    chunk_size = 5\n",
    "    \n",
    "    for i in range(0, len(file_paths), chunk_size):\n",
    "        chunk_files = file_paths[i:i + chunk_size]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(pd.read_parquet, path) for path in chunk_files]\n",
    "            \n",
    "            for future, file_path in zip(as_completed(futures), chunk_files):\n",
    "                try:\n",
    "                    df = future.result()\n",
    "                    if not df.empty:\n",
    "                        speed_data_frames.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    successful_loads = len(speed_data_frames)\n",
    "    print(f\"Loaded {successful_loads} speed parquets\")\n",
    "    \n",
    "    if speed_data_frames:\n",
    "        combined_data = pd.concat(speed_data_frames, ignore_index=True)\n",
    "        print(f\"Combined speed data shape: {combined_data.shape}\")\n",
    "        return combined_data\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation for chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hourly_speeds(speed_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate hourly speeds for each route on each weekday\"\"\"\n",
    "    \n",
    "    # Group and sum distances and times\n",
    "    hourly_totals = speed_data.groupby(['route_id', 'weekday', 'hour']).agg({\n",
    "        'segment_length': 'sum',  # total distance in feet\n",
    "        'time_elapsed': 'sum',    # total time in seconds\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate average speeds\n",
    "    hourly_totals['average_speed_mph'] = (\n",
    "        (hourly_totals['segment_length'] / 5280) /  # convert feet to miles\n",
    "        (hourly_totals['time_elapsed'] / 3600)      # convert seconds to hours\n",
    "    )\n",
    "    \n",
    "    # Select and format final results\n",
    "    hourly_speeds = hourly_totals[[\n",
    "        'route_id', \n",
    "        'weekday', \n",
    "        'hour', \n",
    "        'average_speed_mph'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Round speeds for readability\n",
    "    hourly_speeds['average_speed_mph'] = hourly_speeds['average_speed_mph'].round(2)\n",
    "\n",
    "    print(f\"\\nGenerated {len(hourly_speeds)} hourly speed records\")\n",
    "    print(f\"Routes analyzed: {hourly_speeds['route_id'].nunique()}\")\n",
    "    \n",
    "    return hourly_speeds\n",
    "\n",
    "\n",
    "def batch_process_hourly_speed(\n",
    "    directory_paths: List[str], \n",
    "    start_date: str, \n",
    "    end_date: str, \n",
    "    output_path: str,\n",
    "    max_workers: int = 4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process speed data from multiple directories in parallel\n",
    "    \n",
    "    Args:\n",
    "        directory_paths: List of directories containing speed files\n",
    "        start_date: Start date in YYYYMMDD format\n",
    "        end_date: End date in YYYYMMDD format\n",
    "        output_path: Path to save processed speed data\n",
    "        max_workers: Number of parallel workers\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {len(directory_paths)} directories from {start_date} to {end_date}\")\n",
    "    speed_data_frames = []\n",
    "\n",
    "    for directory_path in directory_paths:\n",
    "        speed_files = find_speed_parquets(directory_path, start_date, end_date)\n",
    "        if speed_files:\n",
    "            speed_data = load_speed_parquet_data(speed_files)\n",
    "            if not speed_data.empty:\n",
    "                speed_data_frames.append(speed_data)\n",
    "\n",
    "    if speed_data_frames:\n",
    "        # Combine all speed data\n",
    "        combined_speeds = pd.concat(speed_data_frames, ignore_index=True)\n",
    "        \n",
    "        # Calculate hourly averages\n",
    "        hourly_speeds = calculate_hourly_speeds(combined_speeds)\n",
    "        \n",
    "        # Save results\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        hourly_speeds.to_parquet(output_path, index=False)\n",
    "        print(f\"\\nSaved hourly speeds to: {output_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo speed data found in any directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_speeds_dir = \"data/raw-speeds\"\n",
    "# dir_lists is what inside data/raw-speeds\n",
    "dir_lists = [\n",
    "        os.path.join(raw_speeds_dir, d) \n",
    "        for d in os.listdir(raw_speeds_dir) \n",
    "        if os.path.isdir(os.path.join(raw_speeds_dir, d))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 12 directories from 2024-12-03 to 2025-01-04\n",
      "Found 24 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202412120015\n",
      "Loaded 24 speed parquets\n",
      "Combined speed data shape: (239889, 12)\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202502170029\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202501230024\n",
      "Found 23 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202412120015\n",
      "Loaded 23 speed parquets\n",
      "Combined speed data shape: (1272, 12)\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202502170011\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202501020130\n",
      "Found 9 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202409090026\n",
      "Loaded 9 speed parquets\n",
      "Combined speed data shape: (89201, 12)\n",
      "Found 29 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202412120006\n",
      "Loaded 29 speed parquets\n",
      "Combined speed data shape: (7966, 12)\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202501020055\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202502170105\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202408290005\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202501020103\n",
      "\n",
      "Generated 515 hourly speed records\n",
      "Routes analyzed: 5\n",
      "\n",
      "Saved hourly speeds to: data/chart-speeds/control_speeds.parquet\n",
      "\n",
      "Processing 12 directories from 2025-01-05 to 2025-02-06\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202412120015\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202502170029\n",
      "Found 14 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202501230024\n",
      "Loaded 14 speed parquets\n",
      "Combined speed data shape: (158100, 12)\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202412120015\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202502170011\n",
      "Found 28 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202501020130\n",
      "Loaded 28 speed parquets\n",
      "Combined speed data shape: (9341, 12)\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202409090026\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202412120006\n",
      "Found 19 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202501020055\n",
      "Loaded 19 speed parquets\n",
      "Combined speed data shape: (206753, 12)\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202502170105\n",
      "Found 0 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202408290005\n",
      "Found 33 speed parquets between 2025-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202501020103\n",
      "Loaded 33 speed parquets\n",
      "Combined speed data shape: (1694, 12)\n",
      "\n",
      "Generated 512 hourly speed records\n",
      "Routes analyzed: 5\n",
      "\n",
      "Saved hourly speeds to: data/chart-speeds/treatment_speeds.parquet\n"
     ]
    }
   ],
   "source": [
    "batch_process_hourly_speed(\n",
    "    directory_paths=dir_lists,\n",
    "    start_date='2024-12-03',\n",
    "    end_date='2025-01-04',\n",
    "    output_path='data/chart-speeds/control_speeds.parquet'\n",
    ")\n",
    "\n",
    "batch_process_hourly_speed(\n",
    "    directory_paths=dir_lists,\n",
    "    start_date='2025-01-05',\n",
    "    end_date='2025-02-06',\n",
    "    output_path='data/chart-speeds/treatment_speeds.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation for Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def get_directories_by_mdb(mdb_id: str) -> List[str] :\n",
    "    # for folder in data/raw-speeds, if folder name like mdb-513.., append it to directory_paths\n",
    "    directory_paths = []\n",
    "    for folder in os.listdir(\"data/raw-speeds\"):\n",
    "        if folder.startswith(mdb_id):\n",
    "            directory_paths.append(os.path.join(\"data/raw-speeds\", folder))\n",
    "    return directory_paths\n",
    "\n",
    "def merge_segment_files(directory_paths: List[str]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Compare segment.geojson files across different feed directories\n",
    "    \n",
    "    Args:\n",
    "        directory_paths: List of directory paths containing segment files\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame containing merged segment data\n",
    "    \"\"\"\n",
    "    segment_gdfs = []\n",
    "    \n",
    "    print(\"\\nCollecting segment files...\")\n",
    "    # Collect all segment files\n",
    "    for directory in directory_paths:\n",
    "        segment_path = os.path.join(directory, 'segments.geojson')\n",
    "        if os.path.exists(segment_path):\n",
    "            try:\n",
    "                gdf = gpd.read_file(segment_path)\n",
    "                segment_gdfs.append(gdf)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading segment file in {os.path.basename(directory)}: {e}\")\n",
    "    \n",
    "    if not segment_gdfs:\n",
    "        raise ValueError(\"No segment files found\")\n",
    "    \n",
    "    if len(segment_gdfs) == 1:\n",
    "        print(\"Only one segment file found, using it as reference\")\n",
    "        return segment_gdfs[0]\n",
    "    \n",
    "    # Compare contents\n",
    "    reference_gdf = segment_gdfs[0]\n",
    "    all_match = all(gdf.equals(reference_gdf) for gdf in segment_gdfs[1:])\n",
    "    \n",
    "    if all_match:\n",
    "        merged_gdf = reference_gdf\n",
    "    \n",
    "    # If files differ, merge them\n",
    "    merged_gdf = pd.concat(segment_gdfs).drop_duplicates()\n",
    "    print(f\"Final merged segments shape: {merged_gdf.shape}\")\n",
    "    \n",
    "    return merged_gdf\n",
    "\n",
    "\n",
    "def calculate_rush_hour_speeds(speed_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate average speeds during morning and evening rush hours.\n",
    "    \n",
    "    Args:\n",
    "        speed_data: DataFrame containing speed data with columns:\n",
    "                   route_id, trip_id, shape_id, stop_id, prev_stop_id, \n",
    "                   segment_length, time_elapsed, speed_mph, route_id,\n",
    "                   datetime_nyc, date, weekday, hour\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with rush hour speed metrics: \n",
    "        route_id, trip_id, shape_id, stop_id, prev_stop_id, \n",
    "        weekday, rush_hour, avg_speed_mph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    required_columns = ['trip_id', 'shape_id', 'stop_id', 'prev_stop_id', \n",
    "                       'segment_length', 'time_elapsed', 'speed_mph', 'route_id',\n",
    "                       'datetime_nyc', 'date', 'weekday', 'hour']\n",
    "    missing_cols = [col for col in required_columns if col not in speed_data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    \n",
    "    # Define rush hours\n",
    "    speed_data['rush_hour'] = 'non_rush'\n",
    "    morning_mask = (speed_data['hour'] >= 7) & (speed_data['hour'] < 10)\n",
    "    evening_mask = (speed_data['hour'] >= 16) & (speed_data['hour'] < 19)\n",
    "    speed_data.loc[morning_mask, 'rush_hour'] = 'morning_rush'\n",
    "    speed_data.loc[evening_mask, 'rush_hour'] = 'evening_rush'\n",
    "\n",
    "    # Group and aggregate\n",
    "    grouped = speed_data.groupby(['route_id', 'stop_id', 'prev_stop_id', 'weekday', 'rush_hour'])\n",
    "    agg_data = grouped.agg(\n",
    "        total_distance=('segment_length', 'sum'),\n",
    "        total_time=('time_elapsed', 'sum'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate speeds (feet/sec to mph)\n",
    "    agg_data['avg_speed_mph'] = (agg_data['total_distance'] / 5280) / (agg_data['total_time'] / 3600)\n",
    "\n",
    "    # Keep only the columns we need\n",
    "    result = agg_data[['route_id', 'stop_id', 'prev_stop_id', \n",
    "                       'weekday', 'rush_hour', 'avg_speed_mph']]\n",
    "    \n",
    "    print(f\"\\nFinal rush hour speeds shape: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "def calculate_speeds_difference(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the difference in average speeds between two dataframes.\n",
    "    \n",
    "    Args:\n",
    "        df1: First DataFrame with speed data\n",
    "        df2: Second DataFrame with speed data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with speed differences between df1 and df2\n",
    "    \"\"\"\n",
    "    # Ensure both dataframes have the same columns\n",
    "    key_columns = ['route_id', 'stop_id', 'prev_stop_id', 'weekday', 'rush_hour']\n",
    "    \n",
    "    # Add suffixes to distinguish the dataframes when merging\n",
    "    merged_df = pd.merge(\n",
    "        df1, df2, \n",
    "        on=key_columns,\n",
    "        how='inner',\n",
    "        suffixes=('_df1', '_df2')\n",
    "    )\n",
    "    \n",
    "    # Calculate the difference in speeds\n",
    "    merged_df['avg_speed_diff'] = merged_df['avg_speed_mph_df1'] - merged_df['avg_speed_mph_df2']\n",
    "    \n",
    "    # Select only the columns we need for the result\n",
    "    result_df = merged_df[key_columns + ['avg_speed_diff']]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def match_speeds_with_segments(speeds_diff: pd.DataFrame, segments_gdf: gpd.GeoDataFrame, route_id: str, weekday: int) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Match speed differences with geographic segments for visualization.\n",
    "    \n",
    "    Args:\n",
    "        speeds_diff: DataFrame with speed differences\n",
    "        segments_gdf: GeoDataFrame with segment geometries\n",
    "        route_id: Route ID to filter by\n",
    "        weekday: Day of week to filter by (0-6, where 0 is Monday)\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with speed differences and segment geometries\n",
    "    \"\"\"\n",
    "    # Filter speeds by route_id and weekday\n",
    "    filtered_speeds = speeds_diff[(speeds_diff['route_id'] == route_id) & \n",
    "                                 (speeds_diff['weekday'] == weekday)]\n",
    "    \n",
    "    # Join segments to speeds on stop_id and prev_stop_id\n",
    "    matched_segments = pd.merge(\n",
    "        filtered_speeds,\n",
    "        segments_gdf,\n",
    "        on=['stop_id', 'prev_stop_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Convert to GeoDataFrame if it's a regular DataFrame\n",
    "    if not isinstance(matched_segments, gpd.GeoDataFrame):\n",
    "        matched_segments = gpd.GeoDataFrame(matched_segments, geometry=segments_gdf.geometry.name)\n",
    "    \n",
    "    # Convert segment_gdf to web mercator projection (EPSG:3857) required for contextily\n",
    "    matched_segments = matched_segments.to_crs(epsg=3857)\n",
    "    print(\"\\nFinal matched segments shape: \", matched_segments.shape)\n",
    "\n",
    "    return matched_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speed_comparison(\n",
    "    mdb_id: str,\n",
    "    route_ids: List[str],\n",
    "    control_start_date: str,\n",
    "    control_end_date: str,\n",
    "    treatment_start_date: str,\n",
    "    treatment_end_date: str,\n",
    "    output_dir: str = 'data/map-speeds'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process and compare bus speeds between control and treatment periods.\n",
    "    \n",
    "    Args:\n",
    "        mdb_id: str, ID of the MDB (e.g., \"mdb-513\")\n",
    "        route_ids: List[str], list of route IDs to process\n",
    "        control_start_date: str, start date for control period (YYYY-MM-DD)\n",
    "        control_end_date: str, end date for control period (YYYY-MM-DD)\n",
    "        treatment_start_date: str, start date for treatment period (YYYY-MM-DD)\n",
    "        treatment_end_date: str, end date for treatment period (YYYY-MM-DD)\n",
    "        output_dir: str, directory to save output files (default: 'data/map-speeds')\n",
    "    \"\"\"\n",
    "    # Get directories and load segments\n",
    "    directory_paths = get_directories_by_mdb(mdb_id)\n",
    "    segments_gdf = merge_segment_files(directory_paths)\n",
    "\n",
    "    # Save the merged segments to a geojson file\n",
    "    segments_gdf.to_file(os.path.join('data/map-segments', f'{mdb_id}_merged_segments.geojson'), driver='GeoJSON')\n",
    "\n",
    "    # Initialize DataFrames\n",
    "    control_data = pd.DataFrame()\n",
    "    treatment_data = pd.DataFrame()\n",
    "\n",
    "    # Load and combine data from all directories\n",
    "    for directory in directory_paths:   \n",
    "        print(f\"\\nProcessing directory: {directory}\")\n",
    "        \n",
    "        # Load control period data\n",
    "        control_speed_files = find_speed_parquets(directory, control_start_date, control_end_date)\n",
    "        control_speed_data = load_speed_parquet_data(control_speed_files)\n",
    "        control_data = pd.concat([control_data, control_speed_data], ignore_index=True)\n",
    "\n",
    "        # Load treatment period data\n",
    "        treatment_speed_files = find_speed_parquets(directory, treatment_start_date, treatment_end_date)\n",
    "        treatment_speed_data = load_speed_parquet_data(treatment_speed_files)\n",
    "        treatment_data = pd.concat([treatment_data, treatment_speed_data], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nControl data shape: {control_data.shape}\")\n",
    "    print(f\"Treatment data shape: {treatment_data.shape}\")\n",
    "\n",
    "    # Calculate rush hour speeds\n",
    "    control_rush_hour_speeds = calculate_rush_hour_speeds(control_data)  \n",
    "    treatment_rush_hour_speeds = calculate_rush_hour_speeds(treatment_data)\n",
    "\n",
    "    # Calculate speed differences\n",
    "    speeds_diff = calculate_speeds_difference(control_rush_hour_speeds, treatment_rush_hour_speeds)\n",
    "\n",
    "    # Process each route and weekday\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for route_id in route_ids:\n",
    "        # Define output file path\n",
    "        output_file = os.path.join(output_dir, f'{mdb_id}_{route_id}_speed_diff.parquet')\n",
    "        \n",
    "        # Filter speeds by route_id\n",
    "        route_speeds_diff = speeds_diff[speeds_diff['route_id'] == route_id]\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            # Append to existing file\n",
    "            existing_data = pd.read_parquet(output_file)\n",
    "            combined_data = pd.concat([existing_data, route_speeds_diff], ignore_index=True)\n",
    "            combined_data.to_parquet(output_file, engine=\"pyarrow\")\n",
    "            print(f\"Appended data to {output_file}\")\n",
    "        else:\n",
    "            # Create new file\n",
    "            route_speeds_diff.to_parquet(output_file, engine=\"pyarrow\", index=False)\n",
    "            print(f\"Created new file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting segment files...\n",
      "Final merged segments shape: (16545, 11)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-513-202412120015\n",
      "Found 24 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202412120015\n",
      "Loaded 24 speed parquets\n",
      "Combined speed data shape: (239889, 12)\n",
      "Found 24 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202412120015\n",
      "Loaded 24 speed parquets\n",
      "Combined speed data shape: (239889, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-513-202501230024\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202501230024\n",
      "Loaded 0 speed parquets\n",
      "Found 14 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202501230024\n",
      "Loaded 14 speed parquets\n",
      "Combined speed data shape: (158100, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-513-202409090026\n",
      "Found 9 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202409090026\n",
      "Loaded 9 speed parquets\n",
      "Combined speed data shape: (89201, 12)\n",
      "Found 31 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202409090026\n",
      "Loaded 31 speed parquets\n",
      "Combined speed data shape: (285096, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-513-202501020055\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202501020055\n",
      "Loaded 0 speed parquets\n",
      "Found 19 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202501020055\n",
      "Loaded 19 speed parquets\n",
      "Combined speed data shape: (206753, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-513-202502170105\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-513-202502170105\n",
      "Loaded 0 speed parquets\n",
      "Found 0 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-513-202502170105\n",
      "Loaded 0 speed parquets\n",
      "\n",
      "Control data shape: (329090, 12)\n",
      "Treatment data shape: (889838, 12)\n",
      "\n",
      "Final rush hour speeds shape: (2961, 6)\n",
      "\n",
      "Final rush hour speeds shape: (2961, 6)\n",
      "Created new file: data/map-speeds/mdb-513_M50_speed_diff.parquet\n",
      "Created new file: data/map-speeds/mdb-513_M102_speed_diff.parquet\n"
     ]
    }
   ],
   "source": [
    "mdb_id = \"mdb-513\"\n",
    "route_ids = [\"M50\", \"M102\"]\n",
    "\n",
    "process_speed_comparison(\n",
    "        mdb_id=mdb_id,\n",
    "        route_ids=route_ids,\n",
    "        control_start_date=\"2024-12-03\",\n",
    "        control_end_date=\"2025-01-04\",\n",
    "        treatment_start_date=\"2024-01-05\",\n",
    "        treatment_end_date=\"2025-02-06\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting segment files...\n",
      "Final merged segments shape: (42670, 11)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-512-202412120015\n",
      "Found 23 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202412120015\n",
      "Loaded 23 speed parquets\n",
      "Combined speed data shape: (1272, 12)\n",
      "Found 23 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202412120015\n",
      "Loaded 23 speed parquets\n",
      "Combined speed data shape: (1272, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-512-202502170011\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202502170011\n",
      "Loaded 0 speed parquets\n",
      "Found 0 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202502170011\n",
      "Loaded 0 speed parquets\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-512-202408290005\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202408290005\n",
      "Loaded 0 speed parquets\n",
      "Found 2 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202408290005\n",
      "Loaded 2 speed parquets\n",
      "Combined speed data shape: (113, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-512-202501020103\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-512-202501020103\n",
      "Loaded 0 speed parquets\n",
      "Found 33 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-512-202501020103\n",
      "Loaded 33 speed parquets\n",
      "Combined speed data shape: (1694, 12)\n",
      "\n",
      "Control data shape: (1272, 12)\n",
      "Treatment data shape: (3079, 12)\n",
      "\n",
      "Final rush hour speeds shape: (63, 6)\n",
      "\n",
      "Final rush hour speeds shape: (63, 6)\n",
      "Created new file: data/map-speeds/mdb-512_B39_speed_diff.parquet\n"
     ]
    }
   ],
   "source": [
    "mdb_id = \"mdb-512\"\n",
    "route_ids = [\"B39\"]\n",
    "\n",
    "process_speed_comparison(\n",
    "        mdb_id=mdb_id,\n",
    "        route_ids=route_ids,\n",
    "        control_start_date=\"2024-12-03\",\n",
    "        control_end_date=\"2025-01-04\",\n",
    "        treatment_start_date=\"2024-01-05\",\n",
    "        treatment_end_date=\"2025-02-06\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting segment files...\n",
      "Final merged segments shape: (20500, 11)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-514-202502170029\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202502170029\n",
      "Loaded 0 speed parquets\n",
      "Found 0 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202502170029\n",
      "Loaded 0 speed parquets\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-514-202501020130\n",
      "Found 0 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202501020130\n",
      "Loaded 0 speed parquets\n",
      "Found 28 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202501020130\n",
      "Loaded 28 speed parquets\n",
      "Combined speed data shape: (9341, 12)\n",
      "\n",
      "Processing directory: data/raw-speeds/mdb-514-202412120006\n",
      "Found 29 speed parquets between 2024-12-03 and 2025-01-04 in data/raw-speeds/mdb-514-202412120006\n",
      "Loaded 29 speed parquets\n",
      "Combined speed data shape: (7966, 12)\n",
      "Found 31 speed parquets between 2024-01-05 and 2025-02-06 in data/raw-speeds/mdb-514-202412120006\n",
      "Loaded 31 speed parquets\n",
      "Combined speed data shape: (8749, 12)\n",
      "\n",
      "Control data shape: (7966, 12)\n",
      "Treatment data shape: (18090, 12)\n",
      "\n",
      "Final rush hour speeds shape: (610, 6)\n",
      "\n",
      "Final rush hour speeds shape: (640, 6)\n",
      "Created new file: data/map-speeds/mdb-514_SIM24_speed_diff.parquet\n",
      "Created new file: data/map-speeds/mdb-514_SIM4X_speed_diff.parquet\n"
     ]
    }
   ],
   "source": [
    "mdb_id = \"mdb-514\"\n",
    "route_ids = [\"SIM24\", \"SIM4X\"]\n",
    "\n",
    "process_speed_comparison(\n",
    "        mdb_id=mdb_id,\n",
    "        route_ids=route_ids,\n",
    "        control_start_date=\"2024-12-03\",\n",
    "        control_end_date=\"2025-01-04\",\n",
    "        treatment_start_date=\"2024-01-05\",\n",
    "        treatment_end_date=\"2025-02-06\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/map-segments/mdb-513_merged_segments.geojson', 'data/map-segments/mdb-514_merged_segments.geojson', 'data/map-segments/mdb-512_merged_segments.geojson']\n",
      "Merged 3 files into data/map-segments/merged_segments.geojson\n",
      "Final merged file contains 10373 segments\n"
     ]
    }
   ],
   "source": [
    "# aggregation\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get all geojson files in the data/map-segments directory\n",
    "geojson_files = glob.glob('data/map-segments/*.geojson')\n",
    "print(geojson_files)\n",
    "\n",
    "# Initialize an empty GeoDataFrame to store the merged data\n",
    "merged_gdf = None\n",
    "\n",
    "# Loop through all geojson files and merge them\n",
    "for geojson_file in geojson_files:\n",
    "    \n",
    "    # Read the geojson file\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    \n",
    "    # If this is the first file, initialize the merged GeoDataFrame\n",
    "    if merged_gdf is None:\n",
    "        merged_gdf = gdf\n",
    "    else:\n",
    "        # Concatenate with the merged GeoDataFrame\n",
    "        merged_gdf = gpd.GeoDataFrame(pd.concat([merged_gdf, gdf], ignore_index=True))\n",
    "\n",
    "# Remove duplicate segments based on geometry and other key attributes\n",
    "# Assuming segments with same prev_stop_id and stop_id are duplicates\n",
    "if merged_gdf is not None and not merged_gdf.empty:\n",
    "    merged_gdf = merged_gdf.drop_duplicates(subset=['prev_stop_id', 'stop_id', 'geometry'])\n",
    "\n",
    "    # Save the merged and deduplicated GeoDataFrame to a new geojson file\n",
    "    merged_gdf.to_file('data/map-segments/merged_segments.geojson', driver='GeoJSON')\n",
    "    \n",
    "    print(f\"Merged {len(geojson_files)} files into data/map-segments/merged_segments.geojson\")\n",
    "    print(f\"Final merged file contains {len(merged_gdf)} segments\")\n",
    "else:\n",
    "    print(\"No geojson files found or all files were empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 parquet files: ['data/map-speeds/SIM4X_speed_diff_segments.parquet', 'data/map-speeds/B39_speed_diff_segments.parquet', 'data/map-speeds/SIM24_speed_diff_segments.parquet', 'data/map-speeds/M102_speed_diff_segments.parquet', 'data/map-speeds/M50_speed_diff_segments.parquet']\n",
      "Merged 5 files into data/map-speeds/merged_speeds_diff.parquet\n",
      "Final merged file contains 207 segments\n"
     ]
    }
   ],
   "source": [
    "# Get all parquet files in the data/map-speeds directory\n",
    "parquet_files = glob.glob('data/map-speeds/*.parquet')\n",
    "print(f\"Found {len(parquet_files)} parquet files: {parquet_files}\")\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_df = None\n",
    "\n",
    "# Loop through all parquet files and merge them\n",
    "for parquet_file in parquet_files:\n",
    "    # Read the parquet file\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Remove geometry column if it exists\n",
    "    if 'geometry' in df.columns:\n",
    "        df = df.drop(columns=['geometry'])\n",
    "    \n",
    "    # If this is the first file, initialize the merged DataFrame\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        # Concatenate with the merged DataFrame\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "# Remove duplicate segments based on key attributes\n",
    "if merged_df is not None and not merged_df.empty:\n",
    "    merged_df = merged_df.drop_duplicates(subset=['prev_stop_id', 'stop_id'])\n",
    "    \n",
    "    # Save the merged and deduplicated DataFrame to a new parquet file\n",
    "    merged_df.to_parquet('data/map-speeds/merged_speeds_diff.parquet')\n",
    "    \n",
    "    print(f\"Merged {len(parquet_files)} files into data/map-speeds/merged_speeds_diff.parquet\")\n",
    "    print(f\"Final merged file contains {len(merged_df)} segments\")\n",
    "else:\n",
    "    print(\"No parquet files found or all files were empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for segments with different coordinates for the same pair of (prev_stop_id, stop_id)\n",
    "def remove_duplicate_segments(mdb_id, route_id):\n",
    "    \n",
    "    # Read the geojson file\n",
    "    file_path = f'data/map-segments/{mdb_id}_merged_segments.geojson'\n",
    "    segments_gdf = gpd.read_file(file_path)\n",
    "    \n",
    "    # Group by prev_stop_id and stop_id\n",
    "    grouped = segments_gdf.groupby(['prev_stop_id', 'stop_id'])\n",
    "    \n",
    "    # Find groups with more than one entry\n",
    "    duplicates = grouped.filter(lambda x: len(x) > 1)\n",
    "    \n",
    "    # If duplicates exist, keep only the first record for each group\n",
    "    if not duplicates.empty:\n",
    "        print(f\"Found {len(duplicates)} duplicate segments. Keeping only the first record for each group.\")\n",
    "        \n",
    "        # Get the indices of the first occurrence of each group\n",
    "        idx = grouped.apply(lambda x: x.index[0])\n",
    "        \n",
    "        # Select only the first record for each group\n",
    "        deduplicated_gdf = segments_gdf.loc[idx]\n",
    "        \n",
    "        # Keep only the specified columns\n",
    "        columns_to_keep = ['stop_id', 'stop_name', 'prev_stop_id', 'prev_stop_name', \n",
    "                           'projected_position', 'prev_projected_position', 'segment_length', 'geometry']\n",
    "        deduplicated_gdf = deduplicated_gdf[columns_to_keep]\n",
    "        \n",
    "        # Save the deduplicated GeoDataFrame\n",
    "        output_path = f'data/map-segments/{mdb_id}_{route_id}_unique_segments.geojson'\n",
    "        deduplicated_gdf.to_file(output_path, driver='GeoJSON')\n",
    "        \n",
    "    else:\n",
    "        print(f\"No duplicate segments found in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42656 duplicate segments. Keeping only the first record for each group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/n7l6fpkd7bzbgn2k868rryv00000gn/T/ipykernel_35796/4291725950.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = grouped.apply(lambda x: x.index[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16539 duplicate segments. Keeping only the first record for each group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/n7l6fpkd7bzbgn2k868rryv00000gn/T/ipykernel_35796/4291725950.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = grouped.apply(lambda x: x.index[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16539 duplicate segments. Keeping only the first record for each group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/n7l6fpkd7bzbgn2k868rryv00000gn/T/ipykernel_35796/4291725950.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = grouped.apply(lambda x: x.index[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20481 duplicate segments. Keeping only the first record for each group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/n7l6fpkd7bzbgn2k868rryv00000gn/T/ipykernel_35796/4291725950.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = grouped.apply(lambda x: x.index[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20481 duplicate segments. Keeping only the first record for each group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/n7l6fpkd7bzbgn2k868rryv00000gn/T/ipykernel_35796/4291725950.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = grouped.apply(lambda x: x.index[0])\n"
     ]
    }
   ],
   "source": [
    "remove_duplicate_segments('mdb-512', 'B39')\n",
    "remove_duplicate_segments('mdb-513', 'M50')\n",
    "remove_duplicate_segments('mdb-513', 'M102')\n",
    "remove_duplicate_segments('mdb-514', 'SIM4X')\n",
    "remove_duplicate_segments('mdb-514', 'SIM24')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
